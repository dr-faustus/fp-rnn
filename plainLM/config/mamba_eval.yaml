beta1: 0.9
beta2: 0.95
cooldown_steps: null
d_model: 1024
d_state: 16
deterministic: false
dtype: float32
eval_every_steps: 10
exp_name: mamba
expand: '2'
fused_optim: true
grad_accumulation_steps: 2
grad_clip: 1.0
log_every_steps: 20
lr: 0.004
lr_end: 1.0e-05
lr_end_pct: null
lr_start: 0.0
model: mamba1
n_layers: 18
num_workers: 4
optim: adamw
out_dir: /fast/fsarnthein/exp/plainLM
print_progress: true
rms_norm: true
sampler: sequential
sampler_seed: null
scheduler: warmup_cosine
seed: 100
seq_len: 2048
steps_budget: 4800
tie_embeddings: false
torch_compile: false
trainset_path: /fast/smovahedi/data/lm/fineweb/fineweb_pretrain_2B_tokens/train
vocab_size: 50257
wandb_dir: /fast/fsarnthein/logs/plainLM/wandb
wandb_project: llm
wandb_run_name: mamba
warmup_steps: 0.1
weight_decay: 0.1

use_wandb: false
over_write: False
eval: True
validset_path: "/fast/fsarnthein/data/lm/codeparrot/codeparrot_valid_16392ctx/train"
resume: True
resume_micro_step: null
resume_exp_name: mamba/job_idx_2/
micro_batch_size: 1
valid_seqlens: [2048, 4096, 6144, 8196, 10240, 12288, 14336, 16392]

save_last_checkpoint: False
save_intermediate_checkpoints: False
save_every_steps: null