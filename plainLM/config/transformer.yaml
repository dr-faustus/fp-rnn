## CONFIG file
## change paths with your own!

deterministic: False
seed: 100

trainset_path: "/fast/smovahedi/data/lm/fineweb/fineweb_pretrain_2B_tokens/train"
vocab_size: 50257
seq_len: 2048
sampler: 'sequential'
sampler_seed: null
num_workers: 4

eval: False
validset_path: null
eval_every_steps: 10

# model
model: 'transformer'
d_model: 1024
expand: '8/3'
n_layers: 9
n_heads: 16
mlp_class: 'glu'
tie_embeddings: False
torch_compile: False

# note: step budget=token_budget/(seq_len * micro_batch_size * grad_accumulation_steps)
steps_budget: 4800

# note: this is micro batch size if grad_accumulation_steps>1
# note: with ddp, effective batch size = batch_size * grad_accumulation_steps * ddp_world_size
micro_batch_size: 16
grad_accumulation_steps: 2

# note: choose between {float32, float16, bfloat16}
# note: float16 data type will automatically use a GradScaler
dtype: 'float32'

optim: 'adamw'
fused_optim: True 
lr: [0.001, 0.002, 0.004, 0.008, 0.016]
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
grad_clip: 1.0

scheduler: 'warmup_cosine'
warmup_steps: 0.1
cooldown_steps: null
lr_start: 0.0
lr_end: 1.e-5
lr_end_pct: null

log_every_steps: 20
print_progress: True

use_wandb: True
wandb_project: 'llm'
wandb_dir: '/fast/smovahedi/logs/llm/'
wandb_run_name: 'transformer'
exp_name: 'transformer'
out_dir: '/fast/smovahedi/exp/llm'
over_write: True

resume: False
resume_micro_step: null
resume_exp_name: null

save_last_checkpoint: True
save_intermediate_checkpoints: False
save_every_steps: null

